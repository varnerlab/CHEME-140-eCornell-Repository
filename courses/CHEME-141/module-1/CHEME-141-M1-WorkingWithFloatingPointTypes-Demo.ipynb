{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3674061c-9231-46cc-bddf-fb5bfddc5d2b",
   "metadata": {},
   "source": [
    "# Demo: A Deeper Dive into Floating Point Types\n",
    "We work with numerical data in many tasks in science, technology, engineering, and mathematics (STEM). Thus, let's explore common floating-point data types, how they are represented in memory, and the precision associated with each type.\n",
    "* __Key (surprising) fact__: All floating point numbers are approximations! A floating-point number uses a _fixed number of bits_, i.e., memory to store a value, so it can only represent a finite set of rational values rather than the continuum of real numbers. Thus, any real number must be rounded to the nearest floating-point value, making every floating-point value an approximation.\n",
    "\n",
    "The typical floating-point number types you will likely encounter in applications are `Float16`, `Float32`, and `Float64`. But what do these numbers mean? For example, what do the `16`, `32`, and `64` mean in `FloatXX`, what precision can these numbers describe, and how are they represented in memory?\n",
    "\n",
    "To answer these questions, we need to establish a few ground truths. First, on regular, i.e., non-quantum hardware, all floating-point numbers are stored as binary numbers, i.e., numbers written in base `2`. Let's start by looking at the representation of base `b` numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55a4bb-8b15-4027-8b7b-617bc8d378b5",
   "metadata": {},
   "source": [
    "## Base b representation of numbers\n",
    "A number in base $b$ is represented by a finite sequence of digits $(d_{n}d_{n-1}\\dots{d_{1}}d_{0})_{b}$ where each digit $d_{i}$ satifies $0\\leq{d_{i}}<b$. The value (in base 10) of a base $b$ number is the positional sum:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\underbrace{(d_{n}d_{n-1}\\dots{d_{1}}d_{0})_{b}}_{\\text{base b}} = \\underbrace{\\sum_{i=0}^{n}d_{i}b^{i}}_{\\text{value in base 10}}\n",
    "\\end{align*}\n",
    "$$\n",
    "Let's use integers for a few examples to better understand this expression (and then we'll move on to floating point numbers). \n",
    "\n",
    "Consider an `Int64` number. We know that memory storage on modern non-quantum hardware is binary, i.e., base $b = 2$; thus, all the digits $d_{i}$ must be $0\\leq{d}_{i}<2$. However, how many digits do we have, i.e., what is the value of $n$? This is the _word size_, i.e., the `64` in `Int64`.\n",
    "* __Hmmm__. Didn't we already see that? Yes, it's the length of the string output from [the `bitstring(...)` method](https://docs.julialang.org/en/v1/base/numbers/#Base.bitstring)! Let's count the number of zero digits and the number of one digits of a test 64-bit integer using the [`count_zeros(...)` method](https://docs.julialang.org/en/v1/base/numbers/#Base.count_zeros) and [`count_ones(...)` method](https://docs.julialang.org/en/v1/base/numbers/#Base.count_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06a48643-7bc4-4bc5-a4f4-5bab011fbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    wordsize = 64; # default word size\n",
    "    x = 24; # Int64 value by default\n",
    "    n = count_zeros(x) + count_ones(x) # this counts 0's and 1's (doesn't give any info about position)\n",
    "    @assert wordsize == n # see https://docs.julialang.org/en/v1/base/base/#Base.@assert\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344415d4-0382-400d-a703-bf5d3d8ac931",
   "metadata": {},
   "source": [
    "The positions of the `0` and `1` values in the binary number give us the number's value. Suppose we get the bit pattern, i.e., the positions of the digits, using [the `bitstring(...)` method](https://docs.julialang.org/en/v1/base/numbers/#Base.bitstring) and save this value in the `s::String` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ddbefd4-d94c-459c-88b6-15ced4a7f639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0000000000000000000000000000000000000000000000000000000100000001\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = let\n",
    "    x = 257; # Int64 value by default\n",
    "    s = bitstring(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41546b8-0507-425d-9aa6-97476e8b21d2",
   "metadata": {},
   "source": [
    "For the binary string $s$, from the right to the left, we add powers of 2, i.e., the $b^{i}$ terms in the sum, for positions whose digit is a `1`. Let's make this more concrete. \n",
    "* __Hypothesis__: We should be able to _process_ the string $s$, i.e., compute the positional sum of $s$, and get back the value of the integer that generated it. To do this, we'll need to use a few tricks that we haven't discussed yet. Don't worry about how we do it; let's just see if our hypothesis is true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "24bd9e3e-4c6d-4f02-b116-dd16fff714dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was your number 257?\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    b = 2; # what base do we have?\n",
    "    s′ = reverse(s) |> collect .|> x-> parse(Int64,x) # fancy. Nothing to see here, move along (for now anyway).\n",
    "    count = 0; # if this works, when we are finished, this should be our original number\n",
    "    for i ∈ eachindex(s′)\n",
    "        dᵢ = s′[i];\n",
    "        count+= (dᵢ)*(b^(i-1)) # Hmmm. This looks like the position sum, but why i -1?\n",
    "    end\n",
    "    println(\"Was your number $(count)?\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3d79-54b6-4498-95e2-5225f4f2c899",
   "metadata": {},
   "source": [
    "## Floating point numbers\n",
    "Now that we have seen how integers are laid out in memory, let's try to understand the memory layout expressions of floating types `Float16`, `Float32`, and `Float64`. However, before we do that, what is the use case for three floating-point types?\n",
    "\n",
    "Using three floating-point types lets us balance precision and resource usage for different applications: \n",
    "* `Float16` (half-precision) minimizes memory footprint at the expense of precision. This type is ideal for large-scale machine learning inference or graphics where fine precision isn’t critical, and we have many values to store in memory.\n",
    "* `Float32` (single-precision) offers a good compromise of speed and accuracy for most numerical and real-time workloads.\n",
    "* `Float64` (double-precision) provides the high precision and wide exponent range needed in scientific computing, simulations, and financial modeling, where rounding errors must be tightly controlled.\n",
    "\n",
    "Hypothetically, suppose we need more precision than `Float64`, are there bigger values? Yes, specialized types in Julia (and Python) let you go above `Float64`. For example, [the `Quadmath.jl` package](https://github.com/JuliaMath/Quadmath.jl) in Julia implements a `Float128` type if you need precision beyond `Float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71210aca-4e0e-457a-b16b-20eb66f7f0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
