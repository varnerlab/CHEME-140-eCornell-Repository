





include("Include.jl")








ascii_char_dictionary = let

    # initialize -
    ascii_char_dictionary = Dict{Int64, Char}(); # storage for index - character map
    ASCII_character_range = range(0,stop=127,step=1) |> collect; # extended ASCII indexes

    # main loop -
    for i ∈ eachindex(ASCII_character_range)
        my_ascii_char_index = ASCII_character_range[i];
        c = convert(Char, my_ascii_char_index) # hmmm. This is an interesting function.
        ascii_char_dictionary[my_ascii_char_index] = c;
    end
    ascii_char_dictionary;
end





let
    ASCII_index_array = keys(ascii_char_dictionary) |> collect |> sort;
    character_table_df = DataFrame();
    for i ∈ eachindex(ASCII_index_array)
        my_ascii_char_index = ASCII_index_array[i];
        c = ascii_char_dictionary[my_ascii_char_index];

        row = (
            i = my_ascii_char_index,
            character = c
        ); # -> what is going on here? This is a cool type called a NamedTuple ...
        push!(character_table_df,row); # push! -> what is going on here?
    end
    pretty_table(character_table_df, tf=tf_simple)
end





'c' |> Int # wow. this seems a little magical. What is actually going on here?? (notice the single quotes)


Int('c') # what?!? We can convert between ASCII characters and Int! 


Char(99)








🌽 = 16; # corn = 16 \:corn:
🍣 = 4; # sushi = 4 \:sushi:





🌽 + 🍣  # addition?


(🌽 * 🍣) # multiplication?





🌽 ≥ 🍣 # logical comparison? (\geq then tab)





C = let
    C = Set{Char}(); # empty set wiht Char types
    push!(C,'A'); # add a `A` to set C 
    push!(C,'B'); # ... `B` ...
    push!(C,'Q'); # ... `Q` ...
    push!(C,'R'); # ... `R` ...
    push!(C,'S'); # ... `S` ...

    C # return
end





c = 🌽; # Do we have corn in the set ℂ?





c ∈ C # ∈ => \in then tab








test_string_ascii = "Test String in Julia (notice the double quotes). Python uses both single and double quotes for Strings. 😒";





character_array_test_string_ascii = test_string_ascii |> collect








test_unicode_char = '🍣' # want to select another Unicode character?





test_char_index = Unicode.julia_chartransform(test_unicode_char) |> Int





hexidecimal_digits_dictionary = let

    hexidecimal_digits_dictionary = Dict{Int,Char}()
    for i ∈ 0:15
        hexidecimal_digits_dictionary[i] = '0' + i |> Char # hmmm: this is whacky; why does this work?
        if (i > 9)
            hexidecimal_digits_dictionary[i] = 'A' + (i - 10) |> Char
        end
    end
    hexidecimal_digits_dictionary
end





my_code_point = let

    q = test_char_index; 
    remainder_array = Array{Int64,1}();
    while (q != 0)
        r = rem(q,16)
        q = div(q,16)
        push!(remainder_array,r)
    end

    my_code_point = "";
    for i ∈ reverse(remainder_array)
        tmp = hexidecimal_digits_dictionary[i];
        my_code_point *= tmp |> Char;
    end

    # left pad with zeros to get a 4-digit code
    my_code_point = lpad(my_code_point, 5, '0') |> x-> "U+"*x
end





fast_test_index = Unicode.codepoint(test_unicode_char) |> x-> convert(Int,x); # for sushi 127843
@assert fast_test_index == test_char_index





codepoint_bitpattern_array = Unicode.codepoint(test_unicode_char) |> bitstring |> collect |> reverse .|> x-> parse(Int, x)





N = length(codepoint_bitpattern_array)
base = 2;
value = 0;
for i ∈ 0:(N-1)
    value += codepoint_bitpattern_array[i+1]*base^(i); # why i + 1
end
value
