





x = 141.72 |> Float32; # why do we need |> Float32?





typeof(x) == Float32 # if Float32, this should be true





d = let

    # initialize -
    bitpattern_dictionary = Dict{Int64,Int64}(); # storage for the 0-based bit pattern
    wordsize = 32; # how many boxes do we have?
    a = bitstring(x) |> reverse |> collect .|> v-> parse(Int64, v) # fancy. Nothing to see here, move along (for now anyway).
    
    # put stuff in the dictionary
    for i ∈ 0:(wordsize-1)
        bitpattern_dictionary[i] = a[i+1];
    end
    bitpattern_dictionary # return to caller
end;





S = let
    S = (-1.0)^(d[31]);
end





calculated_significand_value = let

    # initialize -
    calculated_significand_value = 0.0;
    b = 2.0; # binary, base = 2
    msb = 23; # most significant bit (msb)
    lsb = 1; # least significant bit (lsb)
    significand_range_array = range(lsb,stop=msb,step=1) |> collect; # range of bits to use for the significand

    for i ∈ significand_range_array
        calculated_significand_value += (b^(-i))*d[msb-i]
    end
    calculated_significand_value + 1
end





@assert significand(x) == calculated_significand_value # compare built-in versus our calculated value





E = let

    # initialize -
    calculated_exponent_value = 0.0;
    b = 2.0; # binary, base = 2
    msb = 30; # most significant bit (msb)
    lsb = 23; # least significant bit (lsb)
    exponent_bit_range_array = range(lsb, stop=msb, step = 1) |> collect

    for i ∈ eachindex(exponent_bit_range_array)
        j = exponent_bit_range_array[i]; # remap operation
        calculated_exponent_value += d[j]*(b^(i))
    end
    calculated_exponent_value
end



